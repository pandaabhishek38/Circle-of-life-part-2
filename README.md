# Circle-of-life-part-2

# Problem Statement

For this project, we consider the same environment as that of project 2. We take a graph of 50 nodes connected by random edges. The environment will consist of three entities: the predator, the prey and the agent. The entities can move along the edges of the given graph. The predator pursues the agent while the agent pursues the prey. If the agent catches the prey before it is caught by the predator, the agent wins. Else, if caught by the predator first, the agent loses. The prey will choose a random node and move to it along the edges of the graph. The predator will move in accordance with the distracted predator model from the previous project. The aim of this project is to develop a strategy to catch the prey as efficiently as possible.

# Implementation:

## The Environment:

We generated a graph using Python dictionary consisting of 50 nodes, numbered 0 to 49, connected in a large circle. The agent, prey and the predator move along the edges connecting these nodes. Additional edges were added between the nodes to increase the connectivity of the graph. We first picked a node of degree less than 3. Then we added an edge between it and a node within 5 steps further or behind in the primary loop. The procedure was repeated until no more edges could be added. This sets up the environment of our project.

The playing ground for this project, that is, the graph is generated by calling functions of the Graph class in the submitted code. Entry point of graph generation starts from the create_graph() function. The create_graph function creates a Python dictionary and immediately creates a two-way path from a particular node to both its adjacent nodes. After this step, every node in the graph will have a degree 2.

For adding the third path, the create_graph() calls the add_edge_3() method of the same class. The add_edge_3() function in turn calls another function edge_available() of the same class. The edge_available() function checks all the left and right side 5 neighbouring nodes of the current node, and returns a list of nodes in that span that have a degree less than 3 back to add_edge_3() function. The add_edge_3() function then randomly picks a node from this returned list and creates a new edge. The following process is repeated for every node until no possible edges can be added.

For any graph generated using the above method, all the nodes will first be connected to their adjacent nodes. Thusj, at this point, every node will have a degree 2. Now, for ease, lets say that every node is then connected to a node on its left side that is 2-edges away from the current node. In doing so, if we keep doing this for all the nodes, then eventually the graph generation algorithm will circle back to the node just on the right side of the initial node. At this point, the algorithm will create an edge between the initial node’s left and the right nodes and there won’t be nay more nodes available with degree less than 3. Now, the point to notice is that when we create an edge between any 2 nodes, the degree of not just the current node is increased, but the other nodes degree is increased by 1 as well. So, after connecting every node to its adjacent node, we will only have 50 orders to update, where adding a single edge between two nodes increases the total order count of the graph by 2. Hence, we will only be able to add 25 edges at max.
## The Predator, the prey and the agent:

As mentioned above, our environment consists of three entities - the predator, the prey and the agent. The agent pursues the prey while being pursued by the predator. They all can move along the edges connecting the nodes. If the agent and the prey occupy the same node at any point of time, the agent wins as it has caught the prey. Whereas if the predator and the agent occupy the same node at any point of time, the agent loses as it has been caught by the predator. There is no issue if the predator and the agent occupy the same node. The three entities move in rounds. The agent makes the first move, followed by the prey and then the predator at last. The three players follow the following rules:

**The Prey:** When its the prey’s turn to make a move, it will select any one of its current neighbors or its current cell, uniformly at random. So if at any point, the prey has 3 neighbors. The probability of the prey picking any 1 of them is 1⁄4 and the probability of not leaving its current node is also 1⁄4. The prey will continue this regardless of the motions of the other two players, until the game concludes.

**The Predator:** When its the predator’s turn to make a move, it will look to move closer to the agent. So it will take a look at each of its neighboring nodes and calculate the distance of each node to the node occupied by the agent at that moment. The predator shall then pick the node with the shortest distance to the agent and then move to that node. In case multiple neighboring nodes have the same shortest distance to the agent, it will select uniformly at random. This behavior of the predator is applicable only to agents 1, 2, 3, and 4. However, for agents 5 and up, the predator acts as a distracted predator. In this, the predator either moves following the shortest path to the agent with a probability of 60%, or it randomly moves to any of its neighbors with a. probability of 40%. This setting is called the distracted predator as is applicable to agents 5, 6, 7, 8, 7_b, 8_b.

**The Agent:** The agent will make its move based on the specific strategy it is following. The agents are implemented in 4 different types of information settings. In the first type, the agent knows the exact location of the predator and the prey at every move. In the second one, the agent will know the location of the predator at each move, but it may not know the location of the prey. In the third information setting, the agent will have information on the location of the prey, but it may not know the exact location of the predator. The final setting is the combination of the previous two settings. The agent does not necessarily know the exact location of both the predator and the prey. In each setting, we have developed different strategies to achieve our aim.

In the code, each agent has its own class. All the agent’s classes except agents 1 and 2 have the below two methods:

1) proceed(): Entry point for simulating the entire game
   
2) survey_node(): Surveys the given node to check whether it has prey or the predator in it. We have implemented in differently in separate Agent classes as survey node functions differently for different agents. For agents 1 and 2, there is no need for these as those agents always know the location of both the prey and the predator. For agents 3 and 4, they survey_node() function checks whether a particular node has the prey in it or not. For agents 5 and 6, it checks for the predator. For agents 7, 8, 7_b and 8_b, it checks if either are present or not.

### Agent1, Agent 2, Agent 3, Agent 4:

The agents 1, 2, 3, 4 remain unchanged from part 1 of this project and are included in this project for comparison to the new agent. One additional part of code that is added in the above four agents is that instead of just returning whether the particular agent won or lost the game, it also returns the agent’s final count of steps that it has taken in the game. Again, for comparison with the new utility based agents.

How many distinct states (configurations of predator, agent, prey) are possible in this environment?

Since there are 50 nodes in the graph, and the agent, prey, predator can be in any of the nodes, the total count of all possible states = 50*50*50 = 125,000

What states are easy to determine U∗ for?

U* is easier to calculate for the success case end states where the agent position == prey position. If U* is the minimal expected number of rounds to catch the prey, then the value of U* for these states is equal to 0. It is also easier to calculate U* for states in which agent position is in neighboring node of prey position. Logically speaking, as per the order of traversal (agent moves→prey moves→ predator moves), if it is the agent’s turn to move and it is in the neighboring node of the prey, it’s U* value should be equal to 1. Since we are not in an ideal world, and we do not want to hardcode the U* values any states except the end states, the U* value of these states should be ~1.

How does U∗(s) relate to U∗ of other states, and the actions the agent can take?

In our implementation, we have initialized the U*(s) values and rewards as below:

U*(all states) = 0

reward(success state) = 1

reward(failure state) = -1

reward(other than above states) = 0

Since U*(s) is calculates by multiplying transition probability by previous U*(s) and also adding the reward, the U*(s) for failure states should be greater than the U*(s) of other cases. Adding to that, the U*(s) of success cases) should be greater than U*(s) of non-game-ending cases. Hence, in our approach, we pick the U*(s) of all the neighboring states as per the possible actions, and pick the action with the largest U*(s) value.

# The Complete Information Setting

## Overview:
In this strategy, the agent will examine all its neighbors and choose the best node to move to, based on the undermentioned rules. In this setting, the agent always knows the location of both the prey and the predator. So, there was no need to calculate probability in this case. We used Breadth-First search algorithm to find the shortest path between two nodes. Using this shortest path, the agent decides its next node and the predator decides its next node from this shortest path as well. The agent in this setting chooses its next node as per the below rules in the same order:

• Neighbors that are closer to the prey and farther from the predator

• Neighbors that are closer to the prey and not closer to the predator

• Neighbors that are not farther from the prey and farther from the predator

• Neighbors that are not farther from the prey and not closer to the predator

• Neighbors that are farther from the predator

• Neighbors that are not closer to the predator

• Remain in the same node

Ties are broken randomly.

# Agent U*

## Implementation:

Before proceeding with the game, the agent calls initialize_vals() of the agents_common class to calculate the utility values.

Initialize_vals(): This function is the starting point in calculating the utility values of all the states. It then keeps on looping until the utility values have converged. To check the “error”, we have taken the sum of the new utility value of a given state minus the previous utility value of that state. Then we add the difference in values of all the possible 125,000 states on this 50 node graph to get the final error. We stop the iterations/convergence when the final error goes below 0.0001.

Inside initialize_vals(): There are first, three for loops from (0,49) for each entity(agent, prey, predator) to loop over all the possible states. In our case, count of all possible states is 125,000. The nested loop assigns a reward to the states and initializes utility values of all the states to 0 for all states except the failure case end states. We have initialized utility of those states with -1.

The rewards:

-If the agent caught the prey – success state – reward = 1

-If the predator caught the agent – failure state – reward = -1

-If the agent caught the prey and the predator caught the agent – failure state – reward = -1 Else - normal state – reward = 0

Then in the initialize_vals() function, we calculate for each state, whether a particular node will be in the shortest distance predator’s path if it tries to move towards the agent. This is stored in the class variable path_dict[(predator position,agent position, node being considered)]. This is needed in calculating the transition probabilities in the calc_util_val() function where we calculate the utility values.

If in path, assign value = 1 

Else, assign value = 0

Then, we loop over all possible states and call calc_util_val() of the same class to calculate the utility value of that state. We loop over this entire loop until the sum of the differences of all the states is < 0.0001. If the code exits this loop, utility values of all the states has been calculated and we return.

calc_util_val(): This function takes input the graph, a modified version of the utility graph (since the agent and the prey can stay in the same position, the list of value against the key in this dictionary also includes the key), and the agent, prey, predator positions for every possible state.

In the function, we loop over all the neighboring nodes of the node of current state and apply the bellman-ford formula to calculate the utility value. The loop for the agent and the prey is for all values in the modified graph dictionary, and the loop for the predator is over the normal dictionary and the predator always moves.

Inside the three nested loops, we check the neighboring nodes of the predator. If it is in the path from the predator to the agent, we calculate the transition probability of that state as below:

Transition probability = (0.6 + (0.4 * (1/number of predator’s neighboring nodes))))/(number of possible actions that the agent can take)

For the distracted predator, transition probability of one neighboring state will be greater than other states (when the predator moves) as on state can happen with the probability of 0.4*1/(number of neighboring states). If that state is in the shortest path to the agent, then its transition probability is increased by adding 0.6 to it.

For the rest of the states which are not present in the predator-agent path, transition probability would be as below:

Transition probability = (0.4 * (1/number of predator’s neighboring nodes)))/(number of possible actions that the agent can take)

Notice that the additional term of 0.6 is not present in this case. To calculate the transition probability of the distracted predator in this way, we have earlier calculated path_dict in initialize_vals() function.

After calculating the transition probability, we multiply it with the previous utility value (or initialized utility value in the first iteration) and add all these values.

Then, we plug the summed-up value in the Bell-Ford equation for value iteration to get a list of utility values (argmax not included yet) by doing the below:

action_utility = (reward for that state) + (beta of 0.9 * summed-up term calculated above)

We calculate the above value for all the possible actions the agent can take, that is, for all neighboring and current node of the agent, and append it to a list. We then pick the greatest value of this list which is considered as the final utility value of that state and return to the calling function.

After calculating the utility values, positions are initialized for the agent, prey, predator before calling the proceed() function of that agent’s class to play the game.

proceed(): The proceed function is the main function where the game goes on. In this, then game keeps on looping until any of the below conditions are true:

-Agent catches the prey

-Predator catches the agent

-Agent catches the prey and predator catches the agent

-Count of steps goes above the threshold (set to 300, but can be changed)

Inside the proceed function, the agent calls the move_agent() function of the same object of the agents_common class in which the utility values were calculated to move.

After every action (agent moves, prey moves, predator moves) in the game, we check if any ending condition has been met and we can exit from the game.

move_agent(): This function calls the calc_util_val() in case of the complete environment to fetch the utility values of the neighboring states that can happen by the agent taking an action. The states are:

-(agent current position, prey current position, predator current position)

-(agent moves to neighbor 1 node, prey current position, predator current position) ...

-(agent moves to neighbor n node, prey current position, predator current position) for n neighboring nodes

It then picks the action form the list temp_vals[] that contains a tuple of (action, utility values) of the above states that has the maximum utility value and returns that action.

The return value of the above call to the move_agent() is set as the agent’s new position, and then we check if any game-ending condition has been reached.

We have run our U* agent once for 30 games and for three different maps. In the first run (output file ustr_1mz_30_pos_100_1.txt) the U* got a 100% success rate and never got caught by the predator. The average step count in this case was 7.333 steps.

In the second run (output file ustr_1mz_30_pos_100.txt) the U* got a 100% success rate and never got caught by the predator. The average step count in this case was 7.8 steps.

In the third run (output file ustr_1mz_30_pos_96.6.txt) the U* got a 96.66% success rate and got caught by the predator only once out of 30 games. The average step count in this case was 7.733333333333333 steps.

Averaging out the success rate for this and the step count, we get:

Agent U* success rate: 98.8889%

Agent U* Step count: 7.622 steps

Considering that Agent 1 took around 300 steps to never get a game disbanded (data taken from project 2’s report), it is huge upgrade!

Are there any starting states for which the agent will not be able to capture the prey? What causes this failure?

The states in which the prey position == predator position, the agent should not be able to capture the prey as we have considered the case when prey position == predator position as a failure case with a negative reward while calculating the U*(s) value.

# Agent U* vs Agent 1

We ran 30 games on a single graph on which both the U star and Agent 1 played together simultaneously. The game was stopped whenever either of the agents caught the prey. In case of the predator catching the prey, it gets a little tricky as the distracted predator can choose any neighboring node randomly or move towards the agent following the shortest path. Now, if two agents are on the same map, then just to make this game a little difficult for our U star agent, we implemented the code in such a way that the predator moves towards the utility-based U star agent. If both U star and the agent 1 are in the same node, good for the predator as it will catch both of them! Otherwise, good for the Agent 1. After the 30-game run, we found the below points:

Count of games U str won before agent 2: 14/30

Count of games Agent 2 won before U str: 1/30

Count of games games in which both agents caught prey at the same time: 13/30

Count of times U str agent got caught: 2/30

(Please check “ustr_vs1_1mz_30_pos.txt” file for output)

Now, we take any one of the 14 games where U star agent won before agent 1. 

Predator starts at position 18

Agent U star at 3

Agent 1 at 3

Prey at 43

The graph used in this run looks like this:

![Fig  1 Agent U star vs Agent 1 graph](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/97cfd188-4f40-4940-93f0-b039997c3a89)

A screenshot from the output tells us that the agents diverge at the very first step

![New Note](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/e446d552-fdb9-47bc-9983-e376fd59d7bc)

The agent 1 tried to move away from the predator as much as possible, taking the from 3, 0. Our U start agent also tries to move away, but it still takes the longer path. As visible in the above picture jst under “temp_vals”, we can see that the utility values actually calculated by the agent U start is of order 0.00000000000001, but we stop modifying the u star values when the total summed difference is just under 0.0001. This difference is stopping early must have introduced some error in the utility values of the nodes due to which the U star agent does not take the optimal path. We can cover this up by reducing the allowed error sum, but then our agent will take much longer to converge and compute the utility values. It is a give and take. For the above game, even though this above action is not optimal, but our agent U star still catches the prey before the Agent 1!

# Agent U* vs Agent 2

Similar to what we have done above, e ran 30 games on a single graph on which both the Ustar and Agent 2 played together simultaneously. The game was stopped whenever either of the agents caught the prey. After the 30 game run, we found the below points:

Count of games Ustr won before agent 2: 18/30

Count of games Agent 2 won before Ustr: 0/30

Count of games games in which both agents caught prey at the same time: 10/30 Count of times Ustr agent got caught: 1/30

Now, we take any one of the 18 games where U star agent won before agent 2. Predator starts at position 4

Agent U star at 15

Agent 1 at 15

Prey at 42

The graph used in this run looks like this:

![New Note](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/1de3e7b8-62fd-4f15-b454-5ba399022153)

A screenshot from the output file tell us that the agents diverge at the very first step, that is, they follow the difference routes from the very beginning.

![New Note](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/9b8a2b78-2be2-4634-89d5-22866280df3e)

Since the Agent 2 follows a written set of rules provided in project 2, it tries to run away from the predator and towards the prey since it detects that the predator is closer to the agent 2 than the prey. However, the agent U star goes in the direction of the predator. The ariance in utility values is less as we have kept the positive and negative rewards small. Due to this, I believe the utility values for cases where predator and prey are far away also reduced (also considering the beta = 0.9 taken in the code). Hence, the agent U star moves towards the direction which is relatively closer to the prey, but also to the predator. However, the value of utility values get bigger and bigger when we move closer to the end states. So, the agent U star taken the decision down the line to move to a safer path. It takes some risk in the beginning, but the outcome also pays off! In the above case as well, the utility-based agent was able to catch the prey before the agent 2 at step 9!

Are there states where the U∗ agent and Agent 1 make different choices? The U∗ agent and Agent 2? Visualize such a state, if one exists, and explain why the U∗ agent makes its choice.

# Agent V

## Implementation

To implement agent V, we have implemented a 6-layer Neural Network. The structure of of the code is such that we can easily add or remove layers from the neural network.

Neural Network Model:
Input – Input data used for predicting the output of the input state that is called by Agent V is of shape (1,3)

Output – The predicted output value of the model that is received by calling the predict() function is the utility value of the given input state of shape (1,)

Output – All utility values of the 125,000 states as numpy array of shape (125000, 3)

Model – The neural network structure is as below:
      -Fully Connected input layer of 3 nodes 
      -Activation Layer with Leaky ReLU 
      -Fully Connected layer of 7 nodes
      -Activation Layer with Leaky ReLU 
      -Fully Connected layer of 11 nodes 
      -Activation Layer with Leaky ReLU 
      -Fully Connected layer of 14 nodes 
      -Activation Layer with Leaky ReLU 
      -Fully Connected layer of 7 nodes 
      -Activation Layer with Leaky ReLU 
      -Fully Connected layer of 3 nodes 
      -Activation Layer with Leaky ReLU 
      -Fully Connected output layer of 1 node 
      -Activation Layer with Leaky ReLU

Data – Input X: all the 125,000 possible input states given as a numpy array of shape (125000,
1, 3) for a single graph that was generated. Since input shape is (125000, 1, 3), we implement a for loop to fetch a single input and update the model’s biases and weights according to that. The predict() function that is called from the agent V is used to predict the utility value of any one state that is given as input.

Output Y: It is the utility values of the 125,000 states as numpy array of shape (125000, 3)

Is overfitting an issue here?

We have not divided the dataset into train and test sets. All the training is run on training data as it was discussed that overfitting should not be a problem in our case. The model is trained every time on the graph on which the game would take place. So, overfitting should not be an issue and hence, we have not added any regularization in our model.

Metrics – We have used the Mean Absolute Error to compute the loss value in each epoch

![New Note](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/1850255d-5bde-47d1-a5b4-f17fa492280e)

First, we call the functions described above in agent U star to calculate the utility values for all the 125,000 states by calling the function initialize_vals() of the agents_common class as this will be the output or y_true of the dataset.

Next, calculate_utils() function is called to format the input and output data, and train the neural network model. Calculate_util() model in turn calls the read_data() function, that formats the dictionary of utility values into two separate numpy arrays. One, with only the node positions of the agent, prey, predator (in that order) as input. Another as the utility value of this input state if output. Calculate_utils() then defines the structure of the model that we will use. We have implemented a 6-layer neural network with Leaky ReLU as the activation function. Before Leaky ReLU, we had tried to use tanh() or ReLU(), but the predicted values were getting calculated as negative during training, ReLU was cutting down the values to 0, and that was causing the neural network model to go haywire. Tanh() was just not a clever choice as it spans over a value of -1 to 1, but in our case, the utility value should never come as negative. Hence, we chose not to use that and instead opted to go for the Leaky ReLU, which will give the benefits of ReLU, but additionaly, it will drastically reduce the size of any negative output from a previous Fully Conncted layer to 1% of its value.

The model then calls the fit() function of the same class to train the model on the pre-processed dataset that we have prepared.

The fit() function then over all the 125,000 sample inputs, performs the forward propagation, error calculation, and backward propagation to update the weights and biases. This loop over the entire input can be defined as one iteration or epoch, the code then loops over this epoch until the calculated loss value after the forward propagation of an iteration is <= 0.1. We had trouble converging the loss our model so instead of setting the error value to be < 0.0001 as done in U* agent, we have used 0.01 as threshold. While this helps our model to converge, but the performance of the model is impacted and hence, we could not achieve a great result with the agent V.

After training, the code flow returns to the calling point and the agent V’s proceed() function is called to start the game. In the proceed() function of class Agent_v, the game iterates after action taken by every entity in our environment, until nay game-ending state has been reached. For every action, the agent V calls the move_v_agent() function of the agents_common class to decide the next node and move.

In the move_v_agent() function, predict() function of the neuralnet class is called to fetch the utility value of all the neighboring states that the agent can be in for an action. A list is created with format [[action 1, utility],[action 2, utility]...] and then the action with the largest utility value is chosen to be the agents next action. The action stated above in our list is the node that the agent can move to. So, after deciding the action/node, the move_v_agent() functions returns this value to the calling function which stores this as the agent’s new position.

We have run our V agent once for 30 games on a map. In the run, (output file V_agnt_op_63.3prcnt_30pos_1mz.txt) the agent V got a 63.3% success rate. It won 19 games and got caught by the predator in other 11 games. The average step count for this was for the above runs was 32.4666666666667 steps.

Therefore, we get:

Agent V success rate: 63.3%

Agent V Step count: 32.4666666666667 steps

Considering that Agent 3 took around 300 steps to never get a game disbanded (data taken from project 2’s report), it is huge upgrade! Since we stop converging when the loss goes below just 0. 1, the utility values calculated by the agent has some faults in it. If we reduce the threshold to a very low value, then the agent should perform much better, but training it will take a lot of time.

In comparison to agent U star, the model V performs much worse on paper, but it is due to the fact that we have set different thresholds for the different models. For U star, we stop updating utility values when the error goes below 0.0001, but we do so for the agent V at jus 0.1. Because of this, the agent V’s performance is worse. However, if we reduce the threshold value when we stop training the model, it should perform much better. Since the model for agent V is trained for every game’s graph, the issue of overfitting won’t apply here. On the other hand, we will have to keep training it multiple times for every new graph. As we have not use optimization methods like RMSProp or Adam, getting the exact value of the learning rate right is becomes a big task as we would want the descent to take very small step size around the minima, which is very very small. Due to this, calculating the exact learning rate becomes difficult. A small learning rate would take a lot of time to converge. Whereas a big learning rate will keep on overshooting.

# The Partial Prey Information Setting

In this type of setting, the agent knows the exact location of the predator at every move but does not know the location of the prey. In each move, the agent can pick a node with the maximum probability of it containing the prey, and checks whether the prey is there or not. The agent gains more information about the prey isn’t every time it enters a node and the prey isn’t there. The agent needs to keep track of a belief state for where the prey is, a collection of probabilities for each node that prey is there. With each move, the agent learns something about the prey, and hence the probabilities must be updated every time we learn something new. And every time the prey is known to move, these probabilities must be updated. Two strategies have been developed in this setting.

# Agent U Partial

## Implementation:

Agent U Partial can be thought of as an add-on on the agent U* that will be needed if we do not know the exact location of the prey in our environment. The belief of probabilities of the prey being in a node is calculated (same as done in project 2 for agent 3) and used to update the utility values of all the states that were calculated as per description in agent U* to get the final utilities that can be used for agent U Partial.

Before running the game, the agent U Partial calls the initialize_vals() function of the agents_common class to calculate the utility values for every possible state(max 125,000) in the graph. Then, the proceed() function of the Agent_u_partial class is called.

In the proceed() function, first the pre-survey prey probabiolties are calculated by calling the update_prey_prob_presurvey() function of the agents_common class (same way ti was done in project 2). The agent then surveys the node with the maximum probability of the prey being in that node. According to the result of the survey, the agent updates the dictionary of prey probabilities by calling update_prey_prob_postsurvey() function of the agents_common class.

After calculating the probabilities, surveying, and updating the probabilities, the agent calls the move_agent() function of the agents_common class to move to a new position.

Note: It was unclear to me whether surveying was allowed to agent. I should have asked. But I implemented both versions of the code. To run the agent U partial with surveying, set the survey_allowed flag in the Agent_u_partial class to True. If we want to run the U partial agent without surveying, set the flag to False.

The move_agent() function in this case in turn calls the calc_partial_utility() function of the same class. Setting the predator position as fixed in its current place, agent position to be a list of nodes that it can move to, and prey’s position is considered to be every node in the graph. Then for all these states, we multiply the utility value of that state with the probabilities of the prey for that state, and get the updated utility values for the actual current state and it’s neighboring states which can happen as per the agent’s action. This is stored in a list and is returned back to the move_agent() function. The move_agent() function then picks the action(node) with the highest utility value, and returns that node to the calling function which in turn updated the agent’s position.
Then the prey and the predator move, and after every entity moves, we check if a game-ending state is reached.

We have run our U partial once for 30 games and for three different maps. In the first run (output file “upar_1mz_30pos_90.txt”) the U* got a 90% success rate and got caught by the predator only thrice out of 30 games. The average step count in this case was 18.766666666666666 steps.

In the second run (output file “upar_1mz_30pos_90_1.txt”) the U partial agent got a 90% success rate and got caught by the predator only thrice out of 30 games. The average step count in this case was 24.833333333333332 steps.

In the third run (output file” upar_1mz_30pos_86.7.txt”) the U partial got a 86.6667% success rate and got caught by the predator only four times out of the 30 games. The average step count in this case was 24.133333333333333 steps.

Averaging out the success rate for this and the step count, we get:

Agent U* success rate: 88.8889%

Agent U* Step count: 17.525 steps

Considering that Agent 3 took around 300 steps to never get a game disbanded (data taken from project 2’s report), it is a big upgrade.

Since the average success rate for this model is around 90%, and the agent does not even know where the predator actually is, it seems that it is almost an optimal agent. We cannot call it optimal since it does get caught by the predator a couple of times.

Probability calculation before survey→Same logic that was applied in project 2 for agent 3 to calculate probability before survey

Probability calculation after survey →Same logic that was applied in project 2 for agent 3 to calculate probability after survey

# Agent U* vs U partial

Also, In comparison with U star agent fom above, we can see that the average step count has increased in our U partial agent. Also, the success rate has gone down. But, considering that U star agent knows the position of the pre and the agent U partial does not, the fall in performance of U partial compared to U star is expected. Nevertheless, the agent U partial still performs well!

# Agent U partial vs Agent 3

We ran 30 games on a single graph on which both the U partial and Agent 3 played together simultaneously. The game was stopped whenever either of the agents caught the prey. In case of the predator catching the prey, it gets a little tricky as the distracted predator can choose any neighboring node randomly or move towards the agent following the shortest path. Now, if two agents are on the same map, then just to make this game a little difficult for our U partial agent, we implemented the code in such a way that the predator moves towards the utility-based U partial agent. If both U star and the agent 3 are in the same node, good for the predator as it will catch both of them! Otherwise, good for the Agent 3. After the 30-game run, we found the below points:

Count of games U partial won before agent 3: 16/30

Count of games Agent 3 won before U partial: 7/30

Count of games in which both agents caught prey at the same time: 1/30

Count of times U partial agent got caught: 6/30

Now, we take any one of the 16 games where U partial agent won before agent 3. Predator starts at position 35

Agent U partial at 2

Agent 3 at 2

Prey at 27

The graph used in this run looks like this:

![New Note](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/3494f004-f631-4e16-81f1-c93e9f3206e5)

A screenshot from the output file tells us that the agents diverge after the second step.

![agents_comm_upar_n_3 step_count 2](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/20bdf9ed-cdce-4bd9-b81b-c1812a9f84ab)

Further looking down in the output file (please check “upar_vs3_1mz_30_pos.txt”), we can see that the agents first survey a node, but they cannot find the prey in that node. So, then the agent 3 picks the node with the maximum probability and moves towards that. However, since we have never ever found the prey in this game from surveying, the agent 3 really does not have a good enough idea of where the prey actually is. Its approach can be described as walking on a foggy road. It cannot see much, but it goes. Whereas, the U partial utility agent multiplies the prey porbabilities to the utility values of all the states. The U partial agent has an additional “feature” called the utility value which helps it to move in the right direction. Due to the different “features” that are available to the agents, both the agents take a different approach to move and diverge.

# Agent U partial vs Agent 4

Since we’re only analyzing one case where the utility and earlier project’s agents differ, running this comparison only for 1 game. So, we ran just game on a single graph on which both the U partial and Agent 4 played together simultaneously. The game was stopped whenever either of the agents caught the prey. After the the game run, we found the below points:

Predator starts at position 34 

Agent U partial at 42

Agent 4 at 42

Prey at 47

The graph used in this run looks like this:

![New Note](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/6ddf14c8-0e1a-431b-af53-f44b84e10f8f)

A screenshot from the output file tells us that the agents diverge after the second step.

![New Note](https://github.com/pandaabhishek38/Circle-of-life-part-2/assets/56110423/e3cc52f4-b522-40d1-b56c-9ef9cb94e050)

Since both the agents do not know the probability of the prey, the agent 4 picks randomly and moves towards the predicted prey position. It got lust that the prey is actually in that direction. Not much can be said about the prey at this point as we have gone through only one iteration of probability updates. The utility-based agent decides to stay in place as the utility of the current state might have been the maximum. It looks like a bad approach taken by the utility agent. However, if we look at the final result, the utility-based agent actually catches in this case the prey before agent 4. Although, the prey randomly walks into the utility agent, so it might have been pure luck in this case.
